{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "End2End.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMRhaPh4iEGQ",
        "outputId": "180ae1ed-cec9-4e1e-8e4b-754b49e7f325"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')\n",
        "%cd /content/gdrive/MyDrive/Data Augmentation-XrayImg"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content/gdrive/MyDrive/Data Augmentation-XrayImg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPJflyS-kTQr"
      },
      "source": [
        "#0. Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eCspd0KkXGR",
        "outputId": "5d071990-f733-4037-e924-3ce95140faa5"
      },
      "source": [
        "!pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/{cu_version}/{torch_version}/index.html"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.openmmlab.com/mmcv/dist/{cu_version}/{torch_version}/index.html\n",
            "Collecting mmcv-full\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/d1/6201ac4b55cd5252a35c9890b65d7629629ca37eed09a072dcbfc96a2817/mmcv-full-1.3.0.tar.gz (253kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 11.0MB/s \n",
            "\u001b[?25hCollecting addict\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/00/b08f23b7d7e1e14ce01419a467b583edbb93c6cdb8654e54a9cc579cd61f/addict-2.4.0-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mmcv-full) (1.19.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from mmcv-full) (7.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from mmcv-full) (3.13)\n",
            "Collecting yapf\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5f/0d/8814e79eb865eab42d95023b58b650d01dec6f8ea87fc9260978b1bf2167/yapf-0.31.0-py2.py3-none-any.whl (185kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 21.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: mmcv-full\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wEDow7bkaNf"
      },
      "source": [
        "!git clone https://github.com/open-mmlab/mmdetection.git\n",
        "%cd mmdetection\n",
        "!pip install -r requirementsa/build.txt\n",
        "!pip install -v -e .\n",
        "!mkdir checkpoints/ "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srMx3VbPiQSE"
      },
      "source": [
        "# 1. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9ryDYGAiHW5"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import mmcv\n",
        "from mmdet.apis import init_detector, inference_detector\n",
        "from matplotlib import patches\n",
        "import numpy as np \n",
        "from torchvision import transforms\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kRK7W4xnYrd"
      },
      "source": [
        "# 1.5 Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDdTyFXnna_U"
      },
      "source": [
        "train_annotations_path='../XRAYBoneDataset/annotations/instances_train.json'\n",
        "val_annotations_path='../XRAYBoneDataset/annotations/instances_val.json'\n",
        "test_annotations_path='../XRAYBoneDataset/annotations/instances_test.json'\n",
        "\n",
        "train_images_path='../XRAYBoneDataset/train/JPEGImages'\n",
        "val_images_path='../XRAYBoneDataset/val/JPEGImages'\n",
        "test_images_path='../XRAYBoneDataset/test/JPEGImages'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xj8QPweMnefd"
      },
      "source": [
        "def read_json(path):\n",
        "  with open(path,'r') as f:\n",
        "    data = json.load(f)\n",
        "  return data\n",
        "\n",
        "\n",
        "def write_json(dic,path):\n",
        "  with open(path,'w') as f:\n",
        "    json.dump(dic,f,indent=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1RqwoIcnmTL"
      },
      "source": [
        "data=read_json(train_annotations_path)\n",
        "print(data['categories'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPtth_U2no5Z"
      },
      "source": [
        "!wget -P ./checkpoints/ http://download.openmmlab.com/mmdetection/v2.0/mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10uokDiDnp7U"
      },
      "source": [
        "# TRAIN Seg\n",
        "!python3 ./tools/train.py ../train_xray_segm.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0t-zwD6iWid"
      },
      "source": [
        "# 2. Configs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbQ0-MR6iUtf"
      },
      "source": [
        "data_train_path = '../original/dataDetect_train'\n",
        "data_test_path = '../original/dataDetect_test'\n",
        "train_data_json='../original/train_data.json'\n",
        "test_data_json='../original/test_data.json'\n",
        "config_file = '../train_xray_segm.py'\n",
        "checkpoint_file = './work_dirs/train_xray_segm/latest.pth'\n",
        "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "BATCH_SIZE=4 # CUDA out of memory :(\n",
        "EPOCH_N=5\n",
        "CLASS_N=2\n",
        "WIDTH=300\n",
        "HEIGHT=300"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MIcVStvibjZ"
      },
      "source": [
        "# 3. Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtWpjsGaidQ9"
      },
      "source": [
        "def get_anno(txt_path):\n",
        "    annos=[]\n",
        "    with open(txt_path) as f:\n",
        "      for line in f.readlines():\n",
        "        anno=line[:-1].split(\" \")\n",
        "        anno = [float(num) for num in anno]\n",
        "        annos.append(anno[1:])\n",
        "    return annos\n",
        "\n",
        "\n",
        "def get_offset(annos,image): # for single image\n",
        "  new_annos=[]\n",
        "  for annotation in annos:\n",
        "    bbox = [annotation[0] * image.shape[1], annotation[1] * image.shape[0], \n",
        "            annotation[2] * image.shape[1], annotation[3] * image.shape[0]]\n",
        "    new_annos.append(bbox)\n",
        "  return new_annos\n",
        "\n",
        "\n",
        "def get_path(data_path):\n",
        "  img_paths = glob.glob(os.path.join(data_path,\"*.png\"))\n",
        "  dic={}\n",
        "  for img_path in img_paths:\n",
        "    txt_path = img_path.replace('.png','.txt')\n",
        "    dic[img_path]=get_anno(txt_path)\n",
        "  return dic\n",
        "\n",
        "\n",
        "def my_draw(img_path,anno):\n",
        "  image=cv2.imread(img_path)\n",
        "  print(image.shape)\n",
        "  bboxs=get_offset(anno,image)\n",
        "  for bbox in bboxs:\n",
        "    cv2.rectangle(image, (int(bbox[0] - bbox[2] / 2), int(bbox[1] - bbox[3] / 2)), (int(bbox[0] + bbox[2] / 2), int(bbox[1] + bbox[3] / 2)), (225, 0, 0), 2)\n",
        "  plt.figure(figsize = (10, 10))\n",
        "  plt.imshow(image)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def convert_bin(result):\n",
        "  x=result[1][0][0]\n",
        "  new_result=[]\n",
        "  for i in range(len(x)):\n",
        "    new_x=[]\n",
        "    for j in range(len(x[i])):\n",
        "      if x[i][j]:\n",
        "        new_x.append(1)\n",
        "      else:\n",
        "        new_x.append(0)\n",
        "    new_result.append(new_x)\n",
        "  return new_result # mask\n",
        "      \n",
        "\n",
        "def remove_background(img,mask):\n",
        "  \n",
        "  img=np.transpose(img, (2, 0, 1))\n",
        "  for channel in img:\n",
        "    for i in range(len(channel)):\n",
        "      for j in range(len(channel[i])):\n",
        "        if mask[i][j]==0:\n",
        "          channel[i][j]=0\n",
        "  \n",
        "  return np.transpose(img, (1, 2, 0))\n",
        "\n",
        "\n",
        "def save_new_img(original_img_paths,save_img_folder):\n",
        "  image_paths = glob.glob(os.path.join(original_img_paths,'*.jpg'))\n",
        "  for i,img_path in enumerate(image_paths):\n",
        "    img=cv2.imread(img_path)\n",
        "    result=inference_detector(model, img_path)\n",
        "    new_re=convert_bin(result)\n",
        "    new_re=np.array(new_re)\n",
        "    new_re=remove_background(img,new_re)\n",
        "    clahe_img=equalize_clahe_color(new_re)\n",
        "    save_img_path=os.path.join(save_img_folder,'%d.jpg'%i)\n",
        "    cv2.imwrite(save_img_path,clahe_img)\n",
        "\n",
        "\n",
        "def equalize_clahe_color(img):\n",
        "    cla = cv2.createCLAHE(clipLimit=4.0, tileGridSize=(18, 18))\n",
        "    channels = cv2.split(img)\n",
        "    eq_channels = []\n",
        "    for ch in channels:\n",
        "        eq_channels.append(cla.apply(ch))\n",
        "    eq_image = cv2.merge(eq_channels)\n",
        "    return eq_image\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_sf6-NnjaEe"
      },
      "source": [
        "# test one sample\n",
        "test_dic=read_json(test_data_json)\n",
        "item=next(iter(test_dic.items()))\n",
        "my_draw(item[0],item[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1qihfVMjg2m"
      },
      "source": [
        "# demo img\n",
        "!ls ../\n",
        "\n",
        "seg_model = init_detector(config_file, checkpoint_file, device=device)\n",
        "path = item[0]\n",
        "img=cv2.imread(path)\n",
        "result=inference_detector(seg_model.cpu(), path)\n",
        "new_re=convert_bin(result)\n",
        "new_re=np.array(new_re)\n",
        "new_re=remove_background(img,new_re)\n",
        "new_re=equalize_clahe_color(new_re)\n",
        "plt.imshow(new_re)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25pZ52hppb6Q"
      },
      "source": [
        "# 4. Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTxqmWcHpmx5"
      },
      "source": [
        "\n",
        "class XrayImgDataset(Dataset):\n",
        "  def __init__(self,json_file,phase):\n",
        "    self.data=read_json(json_file)\n",
        "    self.img_paths=list(self.data.keys())\n",
        "    self.bbxs=list(self.data.values())\n",
        "    self.phase=phase\n",
        "    self.transforms=torchvision.transforms.ToTensor()\n",
        "\n",
        "  def convert_offsets(self,bbx,img):\n",
        "    bboxs=get_offset(bbx,img)\n",
        "    new_bbx=[[int(bbox[0] - bbox[2] / 2), int(bbox[1] - bbox[3] / 2),int(bbox[0] + bbox[2] / 2),int(bbox[1] + bbox[3] / 2)] for bbox in bboxs]\n",
        "    return new_bbx\n",
        "\n",
        "    \n",
        "  def __getitem__(self,idx):\n",
        "    img_path=self.img_paths[idx]\n",
        "    img=cv2.imread(img_path)\n",
        "    result=inference_detector(seg_model.cpu(), img_path)\n",
        "    new_re=convert_bin(result)\n",
        "    new_re=np.array(new_re)\n",
        "    new_re=remove_background(img,new_re)\n",
        "    img=equalize_clahe_color(new_re)\n",
        "    bbxs=self.convert_offsets(self.bbxs[idx],img)\n",
        "    img=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # RGB\n",
        "    \n",
        "    for bbx in bbxs:\n",
        "      bbx[0]=bbx[0]/img.shape[1]*WIDTH\n",
        "      bbx[1]=bbx[1]/img.shape[0]*HEIGHT\n",
        "      bbx[2]=bbx[2]/img.shape[1]*WIDTH\n",
        "      bbx[3]=bbx[3]/img.shape[0]*HEIGHT\n",
        "    img=cv2.resize(img,(WIDTH,HEIGHT))\n",
        "    img=img.astype(float)\n",
        "    img=self.transforms(img)\n",
        "    img=img/255\n",
        "\n",
        "    bbxs=torch.FloatTensor(bbxs)\n",
        "    targets={}\n",
        "    targets['boxes']=bbxs\n",
        "    targets['labels']=torch.ones(bbxs.shape[0]).long()\n",
        "    return img,targets\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYoNU_Z9ppL2"
      },
      "source": [
        "train_data=XrayImgDataset(train_data_json,'train')\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "   train_data,\n",
        "   batch_size=BATCH_SIZE,\n",
        "   num_workers=2,\n",
        "   shuffle=True,\n",
        "   collate_fn=lambda x: list(zip(*x)),\n",
        "   )\n",
        "\n",
        "\n",
        "val_data=XrayImgDataset(test_data_json,'val')\n",
        "val_dataloader = torch.utils.data.DataLoader(\n",
        "   val_data,\n",
        "   batch_size=2,\n",
        "   num_workers=2,\n",
        "   shuffle=True,\n",
        "   collate_fn=lambda x: list(zip(*x)),\n",
        "   )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJ7v49wYpxsb"
      },
      "source": [
        "# test one\n",
        "item=iter(train_dataloader).next()\n",
        "print(item[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu7r57adrslY"
      },
      "source": [
        "# 5. Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXviADzYrq38"
      },
      "source": [
        "model=torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "in_features=model.roi_heads.box_predictor.cls_score.in_features\n",
        "print(in_features)\n",
        "model.roi_heads.box_predictor=FastRCNNPredictor(in_features,CLASS_N)\n",
        "model=model.to(device)\n",
        "params=[p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=5e-4)\n",
        "#print(list(model.children()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kybAq3mJLca"
      },
      "source": [
        "best_point=9999\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ghJwheZsFp2"
      },
      "source": [
        "# 6. Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDnqevLxrzDd"
      },
      "source": [
        "model.load_state_dict(torch.load(\"../end2end.pth\"))\n",
        "\n",
        "for epoch in range(EPOCH_N):\n",
        "\tprint(\"Training...\")\n",
        "\ttrain_loss=0\n",
        "\tval_loss=0\n",
        "\tfor imgs,targets in tqdm(train_dataloader):\n",
        "\t\tmodel.train()\n",
        "\t\tmodel=model.double()\n",
        "\t\toptimizer.zero_grad()\n",
        "\t\timgs=list(img.to(device) for img in imgs)\n",
        "\t\ttargets=[{k:v.to(device) for k,v in t.items()} for t in targets]\n",
        "\t\tloss_dict=model(imgs,targets)\n",
        "\t\tlosses=sum(loss for loss in loss_dict.values())   \t\n",
        "\t\tlosses.backward()\n",
        "\t\toptimizer.step()\n",
        "\t\ttrain_loss+=losses.item()\n",
        "\n",
        "\tfor imgs,targets in tqdm(val_dataloader):\n",
        "\t\tmodel.eval()\n",
        "\t\timgs=list(img.to(device) for img in imgs)\n",
        "\t\ttargets=[{k:v.to(device) for k,v in t.items()} for t in targets]\n",
        "\t\tloss_dict_val=model(imgs,targets)\n",
        "\t\tlosses_val=sum(loss for loss in loss_dict.values())\n",
        "\t\tval_loss+=losses_val.item()\n",
        "\t\tif losses_val.item()<best_point:\n",
        "\t\t\tbest_point=losses_val.item()\n",
        "\t\t\ttorch.save(model.state_dict(), \"../end2end.pth\")\n",
        "\n",
        "\t\t#print(losses.item())\n",
        "\n",
        "\tprint(\"------------------------->\", train_loss,val_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fg2kl07csImK"
      },
      "source": [
        "# 7. Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUM1Q32csKGt"
      },
      "source": [
        "model.load_state_dict(torch.load(\"./end2end.pth\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6_nK6LqsQNP"
      },
      "source": [
        "item=iter(val_dataloader)\n",
        "images,targets=item.next()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOlYQH1IsRki"
      },
      "source": [
        "def view(images,labels,k,std=1,mean=0):\n",
        "  figure=plt.figure(figsize=(30,30))\n",
        "  images=list(images)\n",
        "  labels=list(labels)\n",
        "  for i in range(k):\n",
        "    out=torchvision.utils.make_grid(images[i])\n",
        "    inp=out.cpu().numpy().transpose((1,2,0))\n",
        "    inp=np.array(std)*inp+np.array(mean)\n",
        "    inp=np.clip(inp,0,1)  \n",
        "    ax = figure.add_subplot(2,2, i + 1)\n",
        "    ax.imshow(images[i].cpu().numpy().transpose((1,2,0)))\n",
        "    l=labels[i]['boxes'].cpu().numpy()\n",
        "    l[:,2]=l[:,2]-l[:,0]\n",
        "    l[:,3]=l[:,3]-l[:,1]\n",
        "    for j in range(len(l)):\n",
        "      ax.add_patch(patches.Rectangle((l[j][0],l[j][1]),l[j][2],l[j][3],linewidth=2,edgecolor='w',facecolor='none')) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYEw9PJ0sSq8"
      },
      "source": [
        "view(images,targets,2) # truth"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oMX56GMsUMY"
      },
      "source": [
        "images = list(image.to(device) for image in images)\n",
        "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "model.eval()\n",
        "model=model.double()\n",
        "output=model(images)\n",
        "# predict :))\n",
        "with torch.no_grad():\n",
        "    view(images,output,2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4DK5gS8lJmV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}